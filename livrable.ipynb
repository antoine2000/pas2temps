{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation du pas de temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadre de l'étude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce travail a pour objectif l'étude de l'influence du choix du pas de temps dans les méthodes de résolution d'équations différentielles vues en cours.\n",
    "Plus exactement, on se propose de résoudre des problèmes de la forme : $ x' = f(x) $ avec $x : \\mathbb R^n \\longrightarrow \\mathbb R^m $ et $f : \\mathbb R^m \\longrightarrow \\mathbb R^m $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étude s'est faite en deux temps : tout d'abord nous avons considéré un pas de temps fixe et analysé son influence sur les différents schémas numériques. Nous avons également étudié l'importance de l'ordre de ces derniers : en quoi un schéma d'ordre $n$ est-il plus performant qu'un schéma d'ordre inférieur ?\n",
    "Ensuite nous nous sommes intéressés au cas où le pas de temps est variable lors de la résolution de l'équation. Un programme nous a été fourni, nous devions le comprendre et l'illustrer dans des cas pertinents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports de modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons décidé d'insérer ici, une bonne fois pour toutes, les modules python que nous avons utilisé tout au long de l'étude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as ma\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pas de temps fixe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous donnerons nos implémentations de schémas numériques classiques (Euler, Runge-Kutta) et nous analyserons l'influence du choix du pas de temps sur ces derniers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas explicite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet algorithme est classique, nous avons utilisé des vecteurs pour l'utilisation de fonctions agissant sur des espaces de dimension non réduite à 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_euler_explicit(f, x0, t0, dt, t_tot=2):\n",
    "\n",
    "\t''' Cette fonction renvoie la solution à l'équation différentielle dx/dt = f(x) \n",
    "\tavec la méthode d'Euler explicite, pour une durée de t_tot seconde'''\n",
    "\t\n",
    "\tN = int(ma.floor(t_tot/dt))\n",
    "\tt,x = t0,1*x0    # Donne le temps actuel et la position\n",
    "\tT = [t]  # Liste de temps\n",
    "\tX = array(x)  # Liste de positions\n",
    "\tfor _ in range(0,N):\n",
    "\t\tt += dt    # Nouveau temps\n",
    "\t\tx += dt*f(x)  # Nouvelle position\n",
    "\t\tT = vstack([T,t])\n",
    "\t\tX = vstack([X,x])\n",
    "\n",
    "\treturn T, X.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas implicite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet algorithme utilise le **théorème de Banach** appliqué à la fonction **phi**, qui est bien contractante pour $dt$ petit, il faut donc choisir à partir de quand on considère le point fixe recherché avec la fonction **phi** atteint. Nous avons pris le partie de faire 100 itérations de **phi** puis ensuite d'étudier la variation relative en norme de deux itérations consécutives, en s'arrêtant lorsque cette dernière devient inférieure à 5% (c'est le but de la fonction $next\\_step$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_implicite(f, x0, t0, dt, t_tot = 2):\n",
    "\tN = int(ma.floor(t_tot/dt))  # Number of iterations\n",
    "\tt,x = t0,1*x0\t\t\t\t# Current time & current position\n",
    "\tX = array(x)\t\t\t# Liste de positions\n",
    "\tT = [t]\t\t\t\t# Liste de temps\n",
    "\n",
    "\tdef phi(approx):\n",
    "\t\treturn x + dt*f(approx) \n",
    "\tfor _ in range(N):\n",
    "\t\tt += dt\n",
    "\t\tapprox_pos = phi(x)\n",
    "\t\tx = next_step(approx_pos, phi)\n",
    "\t\tT = vstack([T,t])\n",
    "\t\tX = vstack([X,x])\n",
    "\n",
    "\treturn T,X.T\n",
    "\n",
    "def next_step(approx_pos,phi):\n",
    "\tfor _ in range(100):\n",
    "\t\tapprox_pos = phi(1*approx_pos)\n",
    "\ttemp = phi(approx_pos)\n",
    "\twhile linalg.norm(temp-approx_pos)/linalg.norm(approx_pos) > 0.1 :\n",
    "\t\tapprox_pos = 1*temp\n",
    "\t\ttemp = 1*phi(approx_pos)\n",
    "\treturn approx_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runge-Kutta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les méthodes de Runge-Kutta s'appuient sur des techniques d'approximation de dérivées entre les différents points d'évaluation de la solution (les $(t_i)_{i \\in \\mathbb N}$ ). On a ici implémenté deux de ces techniques : celles d'ordre 2 et 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordre 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Runge_Kutta_2(f, x0, t0, dt, t_tot = 2):\n",
    "\n",
    "    N = int(ma.floor(t_tot/dt))  # Number of iterations\n",
    "    t,x = t0,1*x0                # Current time & current position\n",
    "    X = array(x)                 # Liste de positions\n",
    "    T = [t]                      # Liste de temps\n",
    "\n",
    "    for k in range(N):\n",
    "        t += dt\n",
    "        middle_pos = x + dt/2*f(x)  # On approxime la position intermédiaire avec la pente de la dérivée\n",
    "        x += dt*f(middle_pos)       # On utilise la pente en ce point pour approximer l'intégrale entre deux positions\n",
    "\n",
    "        T = vstack([T,t])\n",
    "        X = vstack([X,x])\n",
    "\n",
    "    return T,X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordre 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Runge_Kutta_4(f, x0, t0, dt, t_tot = 2):\n",
    "\n",
    "    N = int(ma.floor(t_tot/dt))  # Number of iterations\n",
    "    t,x = t0,1*x0                # Current time & current position\n",
    "    X = array(x)                 # Liste de positions\n",
    "    T = [t]                      # Liste de temps\n",
    "\n",
    "    for k in range(N):\n",
    "        t += dt\n",
    "        middle_pos_1 = f(x)                      # Cette variante utilise 4 points intermédiaires, avec des poids différents\n",
    "        middle_pos_2 = f(x + dt/2*middle_pos_1)\n",
    "        middle_pos_3 = f(x + dt/2*middle_pos_2)\n",
    "        middle_pos_4 = f(x + dt*middle_pos_3)\n",
    "        x += dt/6*(middle_pos_1 + 2*middle_pos_2 + 2*middle_pos_3 + middle_pos_4)\n",
    "\n",
    "        T = vstack([T,t])\n",
    "        X = vstack([X,x])\n",
    "\n",
    "    return T,X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultat de tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons décidé de comparer à la fois les méthodes entre elles et l'influence des pas de temps, afin de gagner en concision et en clarté. La méthode à pas variables proposée ensuite est aussi testée ici, cela est plus pratique que de remettre tous les tests une seconde fois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarque importante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les profils d'erreur que nous obtenions étaient très \"bruités\" (beaucoup de points parasites). Nous avons donc pris le parti de \"lisser ces erreurs\" en faisant une erreur \"glissante\", prenant le minimum de l'erreur sur les 30 erreurs précédentes. ($e_i = min(\\{ e_j | j \\in [i-29;i] \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction exponentielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction exponentielle est un exemple intéressant car les erreurs faites lors d'une itération s'ajoutent très facilement, on arrive alors vite à des solutions qui divergent trop vite ou pas assez vite par rapport à la bonne solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](tests/fonction_exp/resume.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction carrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](tests/fonction_carre/resume.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](tests/cos_vectorise/resume.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que ces schémas numériques sont tous **consistants** : l'erreur maximale entre la solution approximée et la vraie solution tend vers 0 lorsque le pas de temps diminue aussi vers 0. C'est une propriété essentielle pour n'importe quel schéma numérique.\n",
    "On voit de plus ici que nos implémentations fonctionnent pour des cas scalaires (exponentielle et carrée) et vectoriels (cosinus). Il est cependant à noter que les méthodes d'Euler semblent moins efficaces que celles de Runge-Kutta d'ordre supérieur à 1. La méthode à pas de temps variable, implémentée plus loin, semble beaucoup plus efficace que ces derniers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pas de temps variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons étudié l'influence du pas de temps, nous voulons optimiser le choix de ce dernier en fonction des situations. En effet une zone \"sensible\" où la fonction varie beaucoup nécessite un pas plus petit qu'une zone totalement linéaire. C'est la raison expliquant la variation possible du pas de temps.\n",
    "L'algorithme suivant a été codé dans ce sens : on garde un pas \"d'enregistrement\" constant, c'est-à-dire que la solution reste échantillonnée sur des intervalles de longueue fixe $dt_{max}$, mais un autre pas est introduit : le pas **interne** $p_i$\n",
    "Ce pas interne détermine les sous-intervalles sur lesquelles nous font les intégrations prévues dans la méthode d'Euler explicite. Il est choisi de manière à garder l'erreur d'approximation entre deux échantillonages inférieure à un certain seuil $T_{rel}$ : en effet, si le pas de temps interne $p_i$ donne lieu à une trop grande erreur d'approximation (i.e si $ ||x_{back} - x_{next}||$ est grand) on réduit ce pas, et vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_ivp_euler_explicit_variable_step_test(f, x0, t0, t_f, dtmin = 1e-16, dtmax = 0.01, atol = 1e-6):\n",
    "    dt = dtmax/10; # initial integration step\n",
    "    ts, xs = [t0], [x0]  # storage variables\n",
    "    t = t0\n",
    "    ti = 0  # internal time keeping track of time since latest storage point : must remain below dtmax\n",
    "    x = x0\n",
    "    while ts[-1] < t_f:\n",
    "        while ti < dtmax:\n",
    "            t_next, ti_next, x_next = t + dt, ti + dt, x + dt * f(x)\n",
    "            x_back = x_next - dt * f(x_next)\n",
    "            ratio_abs_error = atol / (linalg.norm(x_back-x)/2)\n",
    "            dt = 0.9 * dt * sqrt(ratio_abs_error)                  # modification du pas de temps\n",
    "            if dt < dtmin:\n",
    "                raise ValueError(\"Time step below minimum\")\n",
    "            elif dt > dtmax/2:                                     # valeur maximale autorisée pour le pas de temps\n",
    "                dt = dtmax/2\n",
    "            t, ti, x = t_next, ti_next, x_next\n",
    "        dt2DT = dtmax - ti # time left to dtmax\n",
    "        t_next, ti_next, x_next = t + dt2DT, 0, x + dt2DT * f(x)\n",
    "        ts = vstack([ts,t_next])\n",
    "        xs = vstack([xs,x_next])\n",
    "        t, ti, x = t_next, ti_next, x_next\n",
    "    return (dts, ts, xs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intérêt de la méthode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode permet d'aller aussi vite qu'un schéma d'euler explicit classique sur des zones \"faciles\" de la solution à approximer, et prend des pas de temps plus petits pour des zones plus sensibles. Nous allons illustrer cela sur les exemples suivants. Ils sont construits de la manière suivante: en haut se trouve le pas de temps interne choisi entre deux évaluations de la solution (entre $t_i$ et $t_{i+1}$) en bas on trouve la solution approximée et la vraie solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation du pas de temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment faire varier le pas en utilisant le fait que $e^{j+1} = O(\\Delta t^2)$ (voir annexe calculs) ?  \n",
    "Nous n'avons pas réussi à justifier la méthode employée avec seulement cette hypothèse. Mais si on fait l'hypothèse que $e^{j+1} = C\\Delta t^2 + O(\\Delta t^3)$ avec $C \\in \\mathbb{R}$  \n",
    "On a alors $e^{j+2} = C\\Delta t_{new}^2 + O(\\Delta t_{new}^3)$  \n",
    "Avec $C = \\frac{e^{j+1}}{\\Delta t^2} + O(\\Delta t)$  \n",
    "Donc $e^{j+2} = \\frac{e^{j+1}}{\\Delta t^2}\\Delta t_{new}^2 + O(\\Delta t\\Delta t_{new}^2) + O(\\Delta t_{new}^3)$  \n",
    "Et $\\Vert \\frac{e^{j+1}}{\\Delta t^2}\\Delta t_{new}^2 \\Vert \\leqslant TOL_{abs} \\Leftrightarrow \\boxed{\\Delta t_{new} \\leqslant \\Delta t \\sqrt{\\frac{TOL_{abs}}{\\Vert e^{j+1} \\Vert}}}$  \n",
    "On passerait alors d'une erreur d'ordre 2 à une erreur d'ordre 3 en le pas de temps, ce qui justifierait la méthode de variation du pas de temps implémentée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit dans l'exemple suivant que le pas de temps choisi est stationnaire (en zoomant on le voit bien). Or on voit que l'erreur d'approximation du cosinus s'amplifie, ce qui veut dire que l'algorithme ne se \"rend pas compte\" que l'erreur grandit. Nous avons donc imposé une tolérance acceptée en fonction du nombre de points que l'on va echantilloné, en se servant de la propriété de l'algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](tests/IVP/cos_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons testé la fonction pour deux valeurs différentes valeurs $a_{tol}$ ($10^{-6}$ ci-dessus et $6*10^{-10}$ ci-dessous, la seconde valant $\\frac{10^{-6}}{nb_{points}}$), Ce qui donne lieu à des pas de temps choisis différents. On voit par exemple qu'il varie ici entre 2.5 et 5 en logarithme en fonction de l'erreur choisie.\n",
    "L'erreur accumulée chute alors fortement ! Elle passe de l'ordre de 0.1 à l'ordre de 0.001.\n",
    "On voit d'ailleurs par ce biais que l'algorithme IVP ne garantit pas que le seuil de tolérance $a_{tol}$ soit toujours respecté entre deux points d'échantillonage de la solution (entre $t_i$ et $t_{i+1}$). \n",
    "En effet, l'algorithme est censé avoir comme propriété le fait que si $x^{i}$ est la vraie solution au temps $t_i$ alors l'erreur absolue au temps $t_{i+1}$ est inférieure à $a_{tol}$. Ceci est censé garantir (par récurrence) que $||x(t_i)-x^{i}|| \\leq i*a_{tol}$.\n",
    "Pourtant, notre $a_{tol}$ était choisi de telle sorte à ce que si cette propriété était vérifiée, on aurait toujours une erreur d'approximation absolue inférieure à $10^{-6}$, ce qui n'est pas le cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](tests/IVP/cos_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponentielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exponentielle est un cas intéressant car l'on s'attend à ce que le pas de temps choisi diminue en continu, car l'erreur calculée par l'algorithme augmentera nécessairement du fait de l'augmentation de $f$, la dérivée de $x$.\n",
    "Les résultats sont en accord avec cela, l'algorithme est de plus en plus lent et on voit bien l'évolution linéaire du temps choisi en logarithme (donc exponentielle sans l'application du logarithme !). Cependant, l'erreur absolue (entre la vraie solution et celle qui est simulée) reste de l'ordre de 0.01 à la fin de la simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](tests/IVP/exp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction carré donne les mêmes résultats que la fonction cosinus. On obtient encore un pas de temps choisi stationnaire et une erreur absolue d'approximation croissante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](tests/IVP/carre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annexe calculs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord on peut écrire plus clairement la formule qui est en fait : $e^{j+1} = (x^j + \\int_{t_{j}}^{t_{j+1}} f(s,\\tilde x(s)) ds) - x^{j+1}\\\\\n",
    "$  \n",
    "Où $\\tilde x$ est la solution de :\n",
    "\n",
    "$  \\begin{equation}\n",
    "\\left\\lbrace\n",
    "\\begin{array}{}\n",
    "\\dot{\\tilde x}(t) = f(t,x)\\\\\n",
    "\\tilde x(t_j) = x^{j}\\\\\n",
    "\\end{array}\\right.\n",
    "\\end{equation}\\\\\n",
    "e^{j+1} = (x^j + \\int_{t_{j}}^{t_{j+1}} f(s,\\tilde x(s)) ds) - x^{j+1} \\\\\n",
    "= (x^j + \\tilde x(t_{j+1}) - \\tilde x(t_j)) - (x^j + \\int_{t_{j}}^{t_{j+1}} f(t_j,x^j) ds)) \\\\\n",
    "= \\tilde x(t_{j+1}) - \\tilde x(t_j) - \\Delta t f(t_j,x^j) \\\\\n",
    "\\text{Et en utilisant un développement de Taylor à l'ordre 2 sur } \\tilde x \\text{ car } f \\in \\mathcal{C}^1 (\\mathbb{R}x\\mathbb{R}^m,\\mathbb{R}^m) \\, et \\, donc \\, \\tilde x \\in \\mathcal{C}^2 (\\mathbb{R}^n,\\mathbb{R}^m) \\, : \\\\\n",
    "= \\dot{\\tilde x}(t_j)\\Delta t + \\frac{\\ddot{\\tilde x}(t_j)}{2}\\Delta t^2- \\Delta t f(t_j,x^j) + O(\\Delta t ^3) \\\\\n",
    "= f(t_j,\\tilde x(t_j))\\Delta t + \\frac{\\partial f}{\\partial t}(t_j,\\tilde x(t_j))\\frac{\\Delta t^2}{2} +f(t_j,\\tilde x(t_j))\\frac{\\partial f}{\\partial x}(t_j,\\tilde x(t_j))\\frac{\\Delta t^2}{2}- \\Delta t f(t_j,x^j) + O(\\Delta t ^3) \\\\\n",
    "= \\frac{\\partial f}{\\partial t}(t_j,\\tilde x(t_j))\\frac{\\Delta t^2}{2} + f(t_j,\\tilde x(t_j))\\frac{\\partial f}{\\partial x}(t_j,\\tilde x(t_j))\\frac{\\Delta t^2}{2} + O(\\Delta t ^3)\\\\\n",
    "= \\frac{\\partial f}{\\partial t}(t_j,x^j)\\frac{\\Delta t^2}{2} + f(t_j,x^j)\\frac{\\partial f}{\\partial x}(t_j,x^j) + O(\\Delta t ^3)\\\\\n",
    "\\text{Or en utilisant un développement de Taylor à l'ordre 1 sur } f \\text{ car } f \\in \\mathcal{C}^1 (\\mathbb{R}x\\mathbb{R}^m,\\mathbb{R}^m) \\, : \\\\\n",
    "f(t_{j+1},x^{j+1}) = f(t_j,x^j) + \\Delta t \\frac{\\partial f}{\\partial t}(t_j,\\tilde x(t_j)) + (x^{j+1} - x^j)\\frac{\\partial f}{\\partial x}(t_j,x^j) + O(\\Delta t^2)\\\\\n",
    "= f(t_j,x^j) + \\Delta t \\frac{\\partial f}{\\partial t}(t_j,\\tilde x(t^j)) + \\Delta t f(t_j,x^j)\\frac{\\partial f}{\\partial x}(t_j,x^j) + O(\\Delta t^2) \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\text{(E1)}\\\\\n",
    "\\text{Donc } f(t_j,x^j)\\frac{\\partial f}{\\partial x}(t_j,x^j)\\frac{\\Delta t^2}{2} = f(t_{j+1},x^{j+1})\\frac{\\Delta t}{2} - f(t_j,x^j)\\frac{\\Delta t}{2} - \\frac{\\partial f}{\\partial t}(t_j,\\tilde x(t_j)) \\frac{\\Delta t^2}{2} + O(\\Delta t ^3)\\\\\n",
    "\\text{Donc } e^{j+1} = f(t_{j+1},x^{j+1})\\frac{\\Delta t}{2} - f(t_j,x^j)\\frac{\\Delta t}{2} + O(\\Delta t ^3) \\\\\n",
    "= \\frac{\\Delta t}{2}(f(t_{j+1},x^{j+1}) - f(t_j,x^j)) + O(\\Delta t ^3)\\\\\n",
    "\\text{Et enfin par inégalité triangulaire : } \\vert \\Vert e^{j+1} \\Vert - \\Vert \\frac{\\Delta t}{2}(f(t_{j+1},x^{j+1}) - f(t_j,x^j)) \\Vert \\vert \\leqslant \\Vert e^{j+1} - \\frac{\\Delta t}{2}(f(t_{j+1},x^{j+1}) - f(t_j,x^j)) \\Vert = O(\\Delta t ^3) \\\\\n",
    "\\text{Donc : } \\boxed{\\Vert e^{j+1} \\Vert = \\frac{\\Delta t}{2} \\Vert (f(t_{j+1},x^{j+1}) - f(t_j,x^j)) \\Vert + O(\\Delta t ^3)}\\\\\n",
    "\\text{D'après (E1) : } (f(t_{j+1},x^{j+1}) - f(t_j,x^j)) = O(\\Delta t) \\\\\n",
    "\\text{Et donc } \\boxed{e^{j+1} = O(\\Delta t^2)}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
